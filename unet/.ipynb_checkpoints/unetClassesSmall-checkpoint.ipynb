{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1169"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, MaxPooling2D, UpSampling2D, Conv2D\n",
    "#from keras.layers import concatenate\n",
    "#from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave, imread\n",
    "from skimage.transform import resize \n",
    "#from seg_noaa import load_train_data, load_test_data\n",
    "from segmodels import dice_coef, dice_coef_loss, double_conv_layer\n",
    "from segmodels import create_model, preprocess_img, preprocess, test_generator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras import backend as K\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 320)\n"
     ]
    }
   ],
   "source": [
    "img_rows = 320#512\n",
    "img_cols = 320#512\n",
    "batch_size = 16\n",
    "nb_epoch = 25\n",
    "print(img_rows, img_cols)\n",
    "data_path = '/home/ubuntu/noaa/darknet/seals/'\n",
    "train_data_path = os.path.join(data_path, 'JPEGImagesBlk')\n",
    "mask_data_path = '/home/ubuntu/noaa/data/mask/classes'\n",
    "smooth = 1.\n",
    "K.image_dim_ordering()\n",
    "classes = 5\n",
    "OUTPUT_MASK_CHANNELS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def double_conv_layer(x, size, dropout, batch_norm):\n",
    "    conv = Convolution2D(size, 3, 3, border_mode='same')(x)\n",
    "    if batch_norm == True:\n",
    "        conv = BatchNormalization(mode=0, axis=1)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Convolution2D(size, 3, 3, border_mode='same')(conv)\n",
    "    if batch_norm == True:\n",
    "        conv = BatchNormalization(mode=0, axis=1)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    if dropout > 0:\n",
    "        conv = Dropout(dropout)(conv)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def create_model(dropout_val=0.05, batch_norm=True):\n",
    "    inputs = Input((3, img_rows, img_cols))\n",
    "    conv1 = double_conv_layer(inputs, 32, dropout_val, batch_norm)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = double_conv_layer(pool1, 64, dropout_val, batch_norm)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = double_conv_layer(pool2, 128, dropout_val, batch_norm)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = double_conv_layer(pool3, 256, dropout_val, batch_norm)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = double_conv_layer(pool4, 512, dropout_val, batch_norm)\n",
    "    pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n",
    "\n",
    "    conv6 = double_conv_layer(pool5, 1024, dropout_val, batch_norm)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(conv6), conv5], mode='concat', concat_axis=1)\n",
    "    conv7 = double_conv_layer(up6, 512, dropout_val, batch_norm)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv7), conv4], mode='concat', concat_axis=1)\n",
    "    conv8 = double_conv_layer(up7, 256, dropout_val, batch_norm)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv8), conv3], mode='concat', concat_axis=1)\n",
    "    conv9 = double_conv_layer(up8, 128, dropout_val, batch_norm)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv9), conv2], mode='concat', concat_axis=1)\n",
    "    conv10 = double_conv_layer(up9, 64, dropout_val, batch_norm)\n",
    "\n",
    "    up10 = merge([UpSampling2D(size=(2, 2))(conv10), conv1], mode='concat', concat_axis=1)\n",
    "    conv11 = double_conv_layer(up10, 32, 0, batch_norm)\n",
    "\n",
    "    conv12 = Convolution2D(OUTPUT_MASK_CHANNELS, 1, 1)(conv11)\n",
    "    conv12 = BatchNormalization(mode=0, axis=1)(conv12)\n",
    "    conv12 = Activation('sigmoid')(conv12)\n",
    "\n",
    "    model = Model(input=inputs, output=conv12)\n",
    "    return model\n",
    "\n",
    "def preprocess_img(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], img_rows, img_cols, 3), dtype=np.float64)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i] = resize(imgs[i], (img_cols, img_rows, 3), preserve_range=True)\n",
    "\n",
    "    imgs_p = imgs_p[..., np.newaxis]\n",
    "    return imgs_p\n",
    "\n",
    "def preprocess_mask(imgs, channels=5):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], img_rows, img_cols, channels), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i] = multi_resize(imgs[i])\n",
    "\n",
    "    imgs_p = imgs_p[..., np.newaxis]\n",
    "    return imgs_p\n",
    "\n",
    "def show_mask(img):\n",
    "    imout = np.zeros((img.shape[0]*2, img.shape[1]*3), dtype=np.uint8)\n",
    "    for i in range(5):\n",
    "        y_pos, x_pos = math.floor(i/3)*img.shape[0], i%3*img.shape[0]\n",
    "        imout[int(y_pos):int((y_pos+img.shape[0])), int(x_pos):int((x_pos+img.shape[0]))] = img[:,:,i]\n",
    "        imout[int(y_pos):int(y_pos)+2,:] = 1\n",
    "        imout[:,int(x_pos):int(x_pos)+2] = 1\n",
    "    plt.imshow(imout)\n",
    "    plt.show()\n",
    "    \n",
    "def multi_resize(img_mask, image_rows=img_rows, image_cols=img_cols, classes=classes):\n",
    "    imout = np.ndarray((image_rows, image_cols, classes), dtype=np.uint8)\n",
    "    for i in range(classes):\n",
    "        imout[:,:,i] = resize(img_mask[:,:,i].astype(np.float32), (img_rows, img_cols), mode='reflect')\n",
    "    return imout\n",
    "\n",
    "def create_train_data(images, classes=5):\n",
    "    total = len(images) \n",
    "    imgs = np.ndarray((total, img_rows, img_cols, 3), dtype=np.float32)\n",
    "    imgs_mask = np.ndarray((total, img_rows, img_cols, classes), dtype=np.uint8)\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    for image_mask_name in images:\n",
    "        image_name = image_mask_name.split('.')[0] + '.jpg'\n",
    "        img = imread(os.path.join(train_data_path, image_name), as_grey=False)\n",
    "        img_mask = np.load(os.path.join(mask_data_path,'train', image_mask_name))\n",
    "        img = resize(img, (img_rows, img_cols), mode='reflect')\n",
    "        img_mask = multi_resize(img_mask)\n",
    "\n",
    "        img = np.array([img])\n",
    "        img_mask = np.array([img_mask])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_mask[i] = img_mask\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "    return imgs, imgs_mask\n",
    "\n",
    "def test_generator(df, input_folder, batch_size = 16):\n",
    "    n = df.shape[0]\n",
    "    batch_index = 0\n",
    "    while 1:\n",
    "        current_index = batch_index * batch_size\n",
    "        if n >= current_index + batch_size:\n",
    "            current_batch_size = batch_size\n",
    "            batch_index += 1    \n",
    "        else:\n",
    "            current_batch_size = n - current_index\n",
    "            batch_index = 0        \n",
    "        batch_df = df[current_index:current_index+current_batch_size]\n",
    "        batch_x = np.zeros((batch_df.shape[0], img_rows, img_cols, 3)).astype('float32')\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            img = imread(os.path.join(data_path, input_folder, row[0]), as_grey=False)\n",
    "            img = resize(img, (img_rows, img_cols), mode='reflect')\n",
    "            x = np.array([img])\n",
    "            x -= mean\n",
    "            x /= std\n",
    "            batch_x[i] = x\n",
    "            i += 1\n",
    "        if batch_index%300 == 0: print(batch_index)\n",
    "        yield(batch_x.transpose(0, 3, 1, 2))\n",
    "\n",
    "def testchk_generator(df, input_folder, batch_size = 16):\n",
    "    n = df.shape[0]\n",
    "    batch_index = 0\n",
    "    while 1:\n",
    "        current_index = batch_index * batch_size\n",
    "        if n >= current_index + batch_size:\n",
    "            current_batch_size = batch_size\n",
    "            batch_index += 1    \n",
    "        else:\n",
    "            current_batch_size = n - current_index\n",
    "            batch_index = 0        \n",
    "        batch_df = df[current_index:current_index+current_batch_size]\n",
    "        batch_x = np.zeros((batch_df.shape[0], img_rows, img_cols, 3)).astype('float32')\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            img = imread(os.path.join(data_path, input_folder, row[0]), as_grey=False)\n",
    "            img = resize(img, (img_rows, img_cols), mode='reflect')\n",
    "            x = np.array([img])\n",
    "            x -= mean\n",
    "            x /= std\n",
    "            batch_x[i] = x\n",
    "            i += 1\n",
    "        if batch_index%300 == 0: print(batch_index)\n",
    "        return(batch_x.transpose(0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7973"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = os.listdir(os.path.join(mask_data_path,'train'))\n",
    "images = [i for i in images if '.npy' in i]\n",
    "img_folds = [[i for i in images if int(i.split('_')[0])%2==0], [i for i in images if int(i.split('_')[0])%2==1]]\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Upload data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "Done: 0/4027 images\n",
      "Done: 500/4027 images\n"
     ]
    }
   ],
   "source": [
    "for fold in range(2):\n",
    "    print('-'*30)\n",
    "    print('Upload data...')\n",
    "    print('-'*30)\n",
    "    imgs_train, imgs_mask_train = create_train_data(img_folds[fold], classes=5)\n",
    "    imgs_train = preprocess_img(imgs_train)\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Process data...')\n",
    "    print('-'*30)\n",
    "    imgs_mask_train = preprocess_mask(imgs_mask_train)\n",
    "    imgs_train = imgs_train.astype('float32')\n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "    imgs_train -= mean\n",
    "    imgs_train /= std\n",
    "\n",
    "    imgs_train = imgs_train[:,:,:,:,0]\n",
    "    imgs_mask_train = imgs_mask_train[:,:,:,:,0]\n",
    "\n",
    "    #print(imgs_mask_train[100].shape)\n",
    "    #print(imgs_mask_train[100].dtype)\n",
    "    #show_mask(imgs_mask_train[100])\n",
    "\n",
    "    #del model\n",
    "    print('-'*30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-'*30)\n",
    "    model = create_model()\n",
    "    model_checkpoint = ModelCheckpoint('weights_class_fold'+str(fold)+'.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Fitting model...')\n",
    "    print('-'*30)\n",
    "    optim = Adam(lr=.001)\n",
    "    model.compile(optimizer=optim, loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    model.fit(imgs_train.transpose(0, 3, 1, 2), imgs_mask_train.transpose(0, 3, 1, 2), batch_size=batch_size, \n",
    "              verbose=1, shuffle=True, nb_epoch=nb_epoch,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[model_checkpoint])\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Predicting masks...')\n",
    "    print('-'*30)\n",
    "    imgs_test, tmp  = create_train_data(img_folds[abs(fold-1)], classes=5)\n",
    "    imgs_test = preprocess_img(imgs_test)\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "\n",
    "    imgs_test -= mean\n",
    "    imgs_test /= std\n",
    "    imgs_test = imgs_test[:,:,:,:,0]\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Predicting masks on test data...')\n",
    "    print('-'*30)\n",
    "    imgs_mask_test = model.predict(imgs_test.transpose(0, 3, 1, 2), batch_size=16, verbose=1)\n",
    "    imgs_mask_test = imgs_mask_test.transpose(0, 2, 3, 1).astype(np.uint8)\n",
    "\n",
    "    print('-' * 30)\n",
    "    print('Saving predicted masks to files...')\n",
    "    print('-' * 30)\n",
    "    pred_dir = os.path.join(mask_data_path, 'traincv')\n",
    "    if not os.path.exists(pred_dir):\n",
    "        os.mkdir(pred_dir)\n",
    "    for image, image_id in zip(imgs_mask_test, img_folds[abs(fold-1)]):\n",
    "        np.save(os.path.join(pred_dir, image_id), image)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
