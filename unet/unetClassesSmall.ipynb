{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1169"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, gc\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, MaxPooling2D, UpSampling2D, Conv2D\n",
    "#from keras.layers import concatenate\n",
    "#from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave, imread\n",
    "from skimage.transform import resize \n",
    "#from seg_noaa import load_train_data, load_test_data\n",
    "from segmodels import dice_coef, dice_coef_loss, double_conv_layer\n",
    "from segmodels import create_model, preprocess_img, preprocess, test_generator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras import backend as K\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 320)\n"
     ]
    }
   ],
   "source": [
    "img_rows = 320#512\n",
    "img_cols = 320#512\n",
    "batch_size = 16\n",
    "nb_epoch = 25\n",
    "print(img_rows, img_cols)\n",
    "data_path = '/home/ubuntu/noaa/darknet/seals/'\n",
    "train_data_path = os.path.join(data_path, 'JPEGImagesBlk')\n",
    "mask_data_path = '/home/ubuntu/noaa/data/mask/classes'\n",
    "smooth = 1.\n",
    "K.image_dim_ordering()\n",
    "classes = 5\n",
    "OUTPUT_MASK_CHANNELS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def double_conv_layer(x, size, dropout, batch_norm):\n",
    "    conv = Convolution2D(size, 3, 3, border_mode='same')(x)\n",
    "    if batch_norm == True:\n",
    "        conv = BatchNormalization(mode=0, axis=1)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    conv = Convolution2D(size, 3, 3, border_mode='same')(conv)\n",
    "    if batch_norm == True:\n",
    "        conv = BatchNormalization(mode=0, axis=1)(conv)\n",
    "    conv = Activation('relu')(conv)\n",
    "    if dropout > 0:\n",
    "        conv = Dropout(dropout)(conv)\n",
    "    return conv\n",
    "\n",
    "\n",
    "def create_model(dropout_val=0.05, batch_norm=True):\n",
    "    inputs = Input((3, img_rows, img_cols))\n",
    "    conv1 = double_conv_layer(inputs, 32, dropout_val, batch_norm)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = double_conv_layer(pool1, 64, dropout_val, batch_norm)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = double_conv_layer(pool2, 128, dropout_val, batch_norm)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = double_conv_layer(pool3, 256, dropout_val, batch_norm)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = double_conv_layer(pool4, 512, dropout_val, batch_norm)\n",
    "    pool5 = MaxPooling2D(pool_size=(2, 2))(conv5)\n",
    "\n",
    "    conv6 = double_conv_layer(pool5, 1024, dropout_val, batch_norm)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(conv6), conv5], mode='concat', concat_axis=1)\n",
    "    conv7 = double_conv_layer(up6, 512, dropout_val, batch_norm)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv7), conv4], mode='concat', concat_axis=1)\n",
    "    conv8 = double_conv_layer(up7, 256, dropout_val, batch_norm)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv8), conv3], mode='concat', concat_axis=1)\n",
    "    conv9 = double_conv_layer(up8, 128, dropout_val, batch_norm)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv9), conv2], mode='concat', concat_axis=1)\n",
    "    conv10 = double_conv_layer(up9, 64, dropout_val, batch_norm)\n",
    "\n",
    "    up10 = merge([UpSampling2D(size=(2, 2))(conv10), conv1], mode='concat', concat_axis=1)\n",
    "    conv11 = double_conv_layer(up10, 32, 0, batch_norm)\n",
    "\n",
    "    conv12 = Convolution2D(OUTPUT_MASK_CHANNELS, 1, 1)(conv11)\n",
    "    conv12 = BatchNormalization(mode=0, axis=1)(conv12)\n",
    "    conv12 = Activation('sigmoid')(conv12)\n",
    "\n",
    "    model = Model(input=inputs, output=conv12)\n",
    "    return model\n",
    "\n",
    "def preprocess_img(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], img_rows, img_cols, 3), dtype=np.float64)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i] = resize(imgs[i], (img_cols, img_rows, 3), preserve_range=True)\n",
    "\n",
    "    imgs_p = imgs_p[..., np.newaxis]\n",
    "    return imgs_p\n",
    "\n",
    "def preprocess_mask(imgs, channels=5):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], img_rows, img_cols, channels), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i] = multi_resize(imgs[i])\n",
    "\n",
    "    imgs_p = imgs_p[..., np.newaxis]\n",
    "    return imgs_p\n",
    "\n",
    "def show_mask(img):\n",
    "    imout = np.zeros((img.shape[0]*2, img.shape[1]*3), dtype=np.uint8)\n",
    "    for i in range(5):\n",
    "        y_pos, x_pos = math.floor(i/3)*img.shape[0], i%3*img.shape[0]\n",
    "        imout[int(y_pos):int((y_pos+img.shape[0])), int(x_pos):int((x_pos+img.shape[0]))] = img[:,:,i]\n",
    "        imout[int(y_pos):int(y_pos)+2,:] = 1\n",
    "        imout[:,int(x_pos):int(x_pos)+2] = 1\n",
    "    plt.imshow(imout)\n",
    "    plt.show()\n",
    "    \n",
    "def multi_resize(img_mask, image_rows=img_rows, image_cols=img_cols, classes=classes):\n",
    "    imout = np.ndarray((image_rows, image_cols, classes), dtype=np.uint8)\n",
    "    for i in range(classes):\n",
    "        imout[:,:,i] = resize(img_mask[:,:,i].astype(np.float32), (img_rows, img_cols), mode='reflect')\n",
    "    return imout\n",
    "\n",
    "def create_train_data(images, classes=5):\n",
    "    total = len(images) \n",
    "    imgs = np.ndarray((total, img_rows, img_cols, 3), dtype=np.float32)\n",
    "    imgs_mask = np.ndarray((total, img_rows, img_cols, classes), dtype=np.uint8)\n",
    "    i = 0\n",
    "    print('-'*30)\n",
    "    print('Creating training images...')\n",
    "    print('-'*30)\n",
    "    for image_mask_name in images:\n",
    "        image_name = image_mask_name.split('.')[0] + '.jpg'\n",
    "        img = imread(os.path.join(train_data_path, image_name), as_grey=False)\n",
    "        img_mask = np.load(os.path.join(mask_data_path,'train', image_mask_name))\n",
    "        img = resize(img, (img_rows, img_cols), mode='reflect')\n",
    "        img_mask = multi_resize(img_mask)\n",
    "\n",
    "        img = np.array([img])\n",
    "        img_mask = np.array([img_mask])\n",
    "\n",
    "        imgs[i] = img\n",
    "        imgs_mask[i] = img_mask\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print('Done: {0}/{1} images'.format(i, total))\n",
    "        i += 1\n",
    "    print('Loading done.')\n",
    "    return imgs, imgs_mask\n",
    "\n",
    "def test_generator(df, input_folder, batch_size = 16):\n",
    "    n = df.shape[0]\n",
    "    batch_index = 0\n",
    "    while 1:\n",
    "        current_index = batch_index * batch_size\n",
    "        if n >= current_index + batch_size:\n",
    "            current_batch_size = batch_size\n",
    "            batch_index += 1    \n",
    "        else:\n",
    "            current_batch_size = n - current_index\n",
    "            batch_index = 0        \n",
    "        batch_df = df[current_index:current_index+current_batch_size]\n",
    "        batch_x = np.zeros((batch_df.shape[0], img_rows, img_cols, 3)).astype('float32')\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            img = imread(os.path.join(data_path, input_folder, row[0]), as_grey=False)\n",
    "            img = resize(img, (img_rows, img_cols), mode='reflect')\n",
    "            x = np.array([img])\n",
    "            x -= mean\n",
    "            x /= std\n",
    "            batch_x[i] = x\n",
    "            i += 1\n",
    "        if batch_index%300 == 0: print(batch_index)\n",
    "        yield(batch_x.transpose(0, 3, 1, 2))\n",
    "\n",
    "def testchk_generator(df, input_folder, batch_size = 16):\n",
    "    n = df.shape[0]\n",
    "    batch_index = 0\n",
    "    while 1:\n",
    "        current_index = batch_index * batch_size\n",
    "        if n >= current_index + batch_size:\n",
    "            current_batch_size = batch_size\n",
    "            batch_index += 1    \n",
    "        else:\n",
    "            current_batch_size = n - current_index\n",
    "            batch_index = 0        \n",
    "        batch_df = df[current_index:current_index+current_batch_size]\n",
    "        batch_x = np.zeros((batch_df.shape[0], img_rows, img_cols, 3)).astype('float32')\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            img = imread(os.path.join(data_path, input_folder, row[0]), as_grey=False)\n",
    "            img = resize(img, (img_rows, img_cols), mode='reflect')\n",
    "            x = np.array([img])\n",
    "            x -= mean\n",
    "            x /= std\n",
    "            batch_x[i] = x\n",
    "            i += 1\n",
    "        if batch_index%300 == 0: print(batch_index)\n",
    "        return(batch_x.transpose(0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7973"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = os.listdir(os.path.join(mask_data_path,'train'))\n",
    "images = [i for i in images if '.npy' in i]\n",
    "img_folds = [[i for i in images if int(i.split('_')[0])%2==0], [i for i in images if int(i.split('_')[0])%2==1]]\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Upload data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "Done: 0/4027 images\n",
      "Done: 500/4027 images\n",
      "Done: 1000/4027 images\n",
      "Done: 1500/4027 images\n",
      "Done: 2000/4027 images\n",
      "Done: 2500/4027 images\n",
      "Done: 3000/4027 images\n",
      "Done: 3500/4027 images\n",
      "Done: 4000/4027 images\n",
      "Loading done.\n",
      "------------------------------\n",
      "Process data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Train on 3221 samples, validate on 806 samples\n",
      "Epoch 1/25\n",
      "3221/3221 [==============================] - 958s - loss: -0.0624 - dice_coef: 0.0624 - val_loss: -0.0715 - val_dice_coef: 0.0715\n",
      "Epoch 2/25\n",
      "3221/3221 [==============================] - 951s - loss: -0.0736 - dice_coef: 0.0736 - val_loss: -0.0817 - val_dice_coef: 0.0817\n",
      "Epoch 3/25\n",
      "3221/3221 [==============================] - 948s - loss: -0.0860 - dice_coef: 0.0860 - val_loss: -0.0668 - val_dice_coef: 0.0668\n",
      "Epoch 4/25\n",
      "3221/3221 [==============================] - 956s - loss: -0.0991 - dice_coef: 0.0991 - val_loss: -0.1068 - val_dice_coef: 0.1068\n",
      "Epoch 5/25\n",
      "3221/3221 [==============================] - 955s - loss: -0.1155 - dice_coef: 0.1155 - val_loss: -0.1182 - val_dice_coef: 0.1182\n",
      "Epoch 6/25\n",
      "3221/3221 [==============================] - 960s - loss: -0.1358 - dice_coef: 0.1358 - val_loss: -0.1515 - val_dice_coef: 0.1515\n",
      "Epoch 7/25\n",
      "3221/3221 [==============================] - 956s - loss: -0.1577 - dice_coef: 0.1577 - val_loss: -0.1257 - val_dice_coef: 0.1257\n",
      "Epoch 8/25\n",
      "3221/3221 [==============================] - 960s - loss: -0.1824 - dice_coef: 0.1824 - val_loss: -0.1916 - val_dice_coef: 0.1916\n",
      "Epoch 9/25\n",
      "3221/3221 [==============================] - 960s - loss: -0.2129 - dice_coef: 0.2129 - val_loss: -0.2316 - val_dice_coef: 0.2316\n",
      "Epoch 10/25\n",
      "3221/3221 [==============================] - 961s - loss: -0.2430 - dice_coef: 0.2430 - val_loss: -0.2462 - val_dice_coef: 0.2462\n",
      "Epoch 11/25\n",
      "3221/3221 [==============================] - 962s - loss: -0.2710 - dice_coef: 0.2710 - val_loss: -0.3064 - val_dice_coef: 0.3064\n",
      "Epoch 12/25\n",
      "3221/3221 [==============================] - 963s - loss: -0.3048 - dice_coef: 0.3048 - val_loss: -0.3472 - val_dice_coef: 0.3472\n",
      "Epoch 13/25\n",
      "3221/3221 [==============================] - 961s - loss: -0.3340 - dice_coef: 0.3340 - val_loss: -0.3511 - val_dice_coef: 0.3511\n",
      "Epoch 14/25\n",
      "3221/3221 [==============================] - 960s - loss: -0.3603 - dice_coef: 0.3603 - val_loss: -0.3647 - val_dice_coef: 0.3647\n",
      "Epoch 15/25\n",
      "3221/3221 [==============================] - 955s - loss: -0.3890 - dice_coef: 0.3890 - val_loss: -0.3389 - val_dice_coef: 0.3389\n",
      "Epoch 16/25\n",
      "3221/3221 [==============================] - 958s - loss: -0.4163 - dice_coef: 0.4163 - val_loss: -0.4046 - val_dice_coef: 0.4046\n",
      "Epoch 17/25\n",
      "3221/3221 [==============================] - 957s - loss: -0.4424 - dice_coef: 0.4424 - val_loss: -0.4465 - val_dice_coef: 0.4465\n",
      "Epoch 18/25\n",
      "3221/3221 [==============================] - 957s - loss: -0.4614 - dice_coef: 0.4614 - val_loss: -0.4580 - val_dice_coef: 0.4580\n",
      "Epoch 19/25\n",
      "3221/3221 [==============================] - 957s - loss: -0.4845 - dice_coef: 0.4845 - val_loss: -0.4683 - val_dice_coef: 0.4683\n",
      "Epoch 20/25\n",
      "3221/3221 [==============================] - 957s - loss: -0.5101 - dice_coef: 0.5101 - val_loss: -0.4971 - val_dice_coef: 0.4971\n",
      "Epoch 21/25\n",
      "3221/3221 [==============================] - 953s - loss: -0.5254 - dice_coef: 0.5254 - val_loss: -0.4915 - val_dice_coef: 0.4915\n",
      "Epoch 22/25\n",
      "3221/3221 [==============================] - 957s - loss: -0.5413 - dice_coef: 0.5413 - val_loss: -0.5495 - val_dice_coef: 0.5495\n",
      "Epoch 23/25\n",
      "3221/3221 [==============================] - 959s - loss: -0.5620 - dice_coef: 0.5620 - val_loss: -0.5512 - val_dice_coef: 0.5512\n",
      "Epoch 24/25\n",
      "3221/3221 [==============================] - 956s - loss: -0.5749 - dice_coef: 0.5749 - val_loss: -0.5677 - val_dice_coef: 0.5677\n",
      "Epoch 25/25\n",
      "3221/3221 [==============================] - 956s - loss: -0.5861 - dice_coef: 0.5861 - val_loss: -0.5705 - val_dice_coef: 0.5705\n",
      "------------------------------\n",
      "Predicting masks...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "Done: 0/3946 images\n",
      "Done: 500/3946 images\n",
      "Done: 1000/3946 images\n",
      "Done: 1500/3946 images\n",
      "Done: 2000/3946 images\n",
      "Done: 2500/3946 images\n",
      "Done: 3000/3946 images\n",
      "Done: 3500/3946 images\n",
      "Loading done.\n",
      "------------------------------\n",
      "Predicting masks on test data...\n",
      "------------------------------\n",
      "3946/3946 [==============================] - 308s   \n",
      "------------------------------\n",
      "Saving predicted masks to files...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Upload data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "Done: 0/3946 images\n",
      "Done: 500/3946 images\n",
      "Done: 1000/3946 images\n",
      "Done: 1500/3946 images\n",
      "Done: 2000/3946 images\n",
      "Done: 2500/3946 images\n",
      "Done: 3000/3946 images\n",
      "Done: 3500/3946 images\n",
      "Loading done.\n",
      "------------------------------\n",
      "Process data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Train on 3156 samples, validate on 790 samples\n",
      "Epoch 1/25\n",
      "3156/3156 [==============================] - 934s - loss: -0.0634 - dice_coef: 0.0634 - val_loss: -0.0742 - val_dice_coef: 0.0742\n",
      "Epoch 2/25\n",
      "3156/3156 [==============================] - 934s - loss: -0.0759 - dice_coef: 0.0759 - val_loss: -0.0724 - val_dice_coef: 0.0724\n",
      "Epoch 3/25\n",
      "3156/3156 [==============================] - 934s - loss: -0.0870 - dice_coef: 0.0870 - val_loss: -0.0675 - val_dice_coef: 0.0675\n",
      "Epoch 4/25\n",
      "3156/3156 [==============================] - 939s - loss: -0.1017 - dice_coef: 0.1017 - val_loss: -0.0933 - val_dice_coef: 0.0933\n",
      "Epoch 5/25\n",
      "3156/3156 [==============================] - 938s - loss: -0.1173 - dice_coef: 0.1173 - val_loss: -0.1170 - val_dice_coef: 0.1170\n",
      "Epoch 6/25\n",
      "3156/3156 [==============================] - 938s - loss: -0.1350 - dice_coef: 0.1350 - val_loss: -0.1446 - val_dice_coef: 0.1446\n",
      "Epoch 7/25\n",
      "3156/3156 [==============================] - 935s - loss: -0.1563 - dice_coef: 0.1563 - val_loss: -0.1390 - val_dice_coef: 0.1390\n",
      "Epoch 8/25\n",
      "3156/3156 [==============================] - 939s - loss: -0.1804 - dice_coef: 0.1804 - val_loss: -0.1539 - val_dice_coef: 0.1539\n",
      "Epoch 9/25\n",
      "3156/3156 [==============================] - 940s - loss: -0.2052 - dice_coef: 0.2052 - val_loss: -0.2292 - val_dice_coef: 0.2292\n",
      "Epoch 10/25\n",
      "3156/3156 [==============================] - 943s - loss: -0.2346 - dice_coef: 0.2346 - val_loss: -0.2589 - val_dice_coef: 0.2589\n",
      "Epoch 11/25\n",
      "3156/3156 [==============================] - 940s - loss: -0.2686 - dice_coef: 0.2686 - val_loss: -0.2716 - val_dice_coef: 0.2716\n",
      "Epoch 12/25\n",
      "3156/3156 [==============================] - 942s - loss: -0.2977 - dice_coef: 0.2977 - val_loss: -0.3177 - val_dice_coef: 0.3177\n",
      "Epoch 13/25\n",
      "3156/3156 [==============================] - 940s - loss: -0.3267 - dice_coef: 0.3267 - val_loss: -0.3244 - val_dice_coef: 0.3244\n",
      "Epoch 14/25\n",
      "3156/3156 [==============================] - 931s - loss: -0.3531 - dice_coef: 0.3531 - val_loss: -0.3700 - val_dice_coef: 0.3700\n",
      "Epoch 15/25\n",
      "3156/3156 [==============================] - 931s - loss: -0.3871 - dice_coef: 0.3871 - val_loss: -0.3969 - val_dice_coef: 0.3969\n",
      "Epoch 16/25\n",
      "3156/3156 [==============================] - 928s - loss: -0.4058 - dice_coef: 0.4058 - val_loss: -0.3517 - val_dice_coef: 0.3517\n",
      "Epoch 17/25\n",
      "3156/3156 [==============================] - 931s - loss: -0.4351 - dice_coef: 0.4351 - val_loss: -0.4131 - val_dice_coef: 0.4131\n",
      "Epoch 18/25\n",
      "3156/3156 [==============================] - 929s - loss: -0.4598 - dice_coef: 0.4598 - val_loss: -0.1181 - val_dice_coef: 0.1181\n",
      "Epoch 19/25\n",
      "3156/3156 [==============================] - 931s - loss: -0.4692 - dice_coef: 0.4692 - val_loss: -0.4897 - val_dice_coef: 0.4897\n",
      "Epoch 20/25\n",
      "3156/3156 [==============================] - 927s - loss: -0.4903 - dice_coef: 0.4903 - val_loss: -0.4262 - val_dice_coef: 0.4262\n",
      "Epoch 21/25\n",
      "3156/3156 [==============================] - 928s - loss: -0.5101 - dice_coef: 0.5101 - val_loss: -0.4506 - val_dice_coef: 0.4506\n",
      "Epoch 22/25\n",
      "3156/3156 [==============================] - 930s - loss: -0.5312 - dice_coef: 0.5312 - val_loss: -0.4702 - val_dice_coef: 0.4702\n",
      "Epoch 23/25\n",
      "3156/3156 [==============================] - 933s - loss: -0.5489 - dice_coef: 0.5489 - val_loss: -0.5117 - val_dice_coef: 0.5117\n",
      "Epoch 24/25\n",
      "3156/3156 [==============================] - 927s - loss: -0.5686 - dice_coef: 0.5686 - val_loss: -0.4814 - val_dice_coef: 0.4814\n",
      "Epoch 25/25\n",
      "3156/3156 [==============================] - 932s - loss: -0.5772 - dice_coef: 0.5772 - val_loss: -0.5611 - val_dice_coef: 0.5611\n",
      "------------------------------\n",
      "Predicting masks...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "Done: 0/4027 images\n",
      "Done: 500/4027 images\n",
      "Done: 1000/4027 images\n",
      "Done: 1500/4027 images\n",
      "Done: 2000/4027 images\n",
      "Done: 2500/4027 images\n",
      "Done: 3000/4027 images\n",
      "Done: 3500/4027 images\n",
      "Done: 4000/4027 images\n",
      "Loading done.\n",
      "------------------------------\n",
      "Predicting masks on test data...\n",
      "------------------------------\n",
      "4027/4027 [==============================] - 313s   \n",
      "------------------------------\n",
      "Saving predicted masks to files...\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for fold in range(2):\n",
    "    print('-'*30)\n",
    "    print('Upload data...')\n",
    "    print('-'*30)\n",
    "    imgs_train, imgs_mask_train = create_train_data(img_folds[fold], classes=5)\n",
    "    imgs_train = preprocess_img(imgs_train)\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Process data...')\n",
    "    print('-'*30)\n",
    "    imgs_mask_train = preprocess_mask(imgs_mask_train)\n",
    "    imgs_train = imgs_train.astype('float32')\n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "    imgs_train -= mean\n",
    "    imgs_train /= std\n",
    "\n",
    "    imgs_train = imgs_train[:,:,:,:,0]\n",
    "    imgs_mask_train = imgs_mask_train[:,:,:,:,0]\n",
    "\n",
    "    #print(imgs_mask_train[100].shape)\n",
    "    #print(imgs_mask_train[100].dtype)\n",
    "    #show_mask(imgs_mask_train[100])\n",
    "\n",
    "    #del model\n",
    "    print('-'*30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-'*30)\n",
    "    model = create_model()\n",
    "    model_checkpoint = ModelCheckpoint('weights_class_fold'+str(fold)+'.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Fitting model...')\n",
    "    print('-'*30)\n",
    "    optim = Adam(lr=.001)\n",
    "    model.compile(optimizer=optim, loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    model.fit(imgs_train.transpose(0, 3, 1, 2), imgs_mask_train.transpose(0, 3, 1, 2), batch_size=batch_size, \n",
    "              verbose=1, shuffle=True, nb_epoch=nb_epoch,\n",
    "              validation_split=0.2,\n",
    "              callbacks=[model_checkpoint])\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Predicting masks...')\n",
    "    print('-'*30)\n",
    "    imgs_test, tmp  = create_train_data(img_folds[abs(fold-1)], classes=5)\n",
    "    imgs_test = preprocess_img(imgs_test)\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "\n",
    "    imgs_test -= mean\n",
    "    imgs_test /= std\n",
    "    imgs_test = imgs_test[:,:,:,:,0]\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Predicting masks on test data...')\n",
    "    print('-'*30)\n",
    "    imgs_mask_test = model.predict(imgs_test.transpose(0, 3, 1, 2), batch_size=16, verbose=1)\n",
    "    imgs_mask_test = imgs_mask_test.transpose(0, 2, 3, 1).astype(np.uint8)\n",
    "\n",
    "    print('-' * 30)\n",
    "    print('Saving predicted masks to files...')\n",
    "    print('-' * 30)\n",
    "    pred_dir = os.path.join(mask_data_path, 'traincv')\n",
    "    if not os.path.exists(pred_dir):\n",
    "        os.mkdir(pred_dir)\n",
    "    for image, image_id in zip(imgs_mask_test, img_folds[abs(fold-1)]):\n",
    "        np.save(os.path.join(pred_dir, image_id), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Upload data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating training images...\n",
      "------------------------------\n",
      "Done: 0/7973 images\n",
      "Done: 500/7973 images\n",
      "Done: 1000/7973 images\n"
     ]
    }
   ],
   "source": [
    "print('-'*30)\n",
    "print('Upload data...')\n",
    "print('-'*30)\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "imgs_train, imgs_mask_train = create_train_data(images, classes=5)\n",
    "imgs_train = preprocess_img(imgs_train)\n",
    "\n",
    "print('-'*30)\n",
    "print('Process data...')\n",
    "print('-'*30)\n",
    "imgs_mask_train = preprocess_mask(imgs_mask_train)\n",
    "imgs_train = imgs_train.astype('float32')\n",
    "mean = np.mean(imgs_train)  # mean for data centering\n",
    "std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "imgs_train -= mean\n",
    "imgs_train /= std\n",
    "\n",
    "imgs_train = imgs_train[:,:,:,:,0]\n",
    "imgs_mask_train = imgs_mask_train[:,:,:,:,0]\n",
    "\n",
    "#print(imgs_mask_train[100].shape)\n",
    "#print(imgs_mask_train[100].dtype)\n",
    "#show_mask(imgs_mask_train[100])\n",
    "\n",
    "#del model\n",
    "print('-'*30)\n",
    "print('Creating and compiling model...')\n",
    "print('-'*30)\n",
    "model = create_model()\n",
    "model_checkpoint = ModelCheckpoint('weights_class.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "print('-'*30)\n",
    "print('Fitting model...')\n",
    "print('-'*30)\n",
    "optim = Adam(lr=.001)\n",
    "model.compile(optimizer=optim, loss=dice_coef_loss, metrics=[dice_coef])\n",
    "model.fit(imgs_train.transpose(0, 3, 1, 2), imgs_mask_train.transpose(0, 3, 1, 2), batch_size=batch_size, \n",
    "          verbose=1, shuffle=True, nb_epoch=nb_epoch,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[model_checkpoint])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
