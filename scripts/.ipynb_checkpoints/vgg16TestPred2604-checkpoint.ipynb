{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import gc, math\n",
    "import pickle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vgg_std16_model(img_rows, img_cols, channel=1, num_class=None):\n",
    "    \"\"\"\n",
    "    VGG 16 Model for Keras\n",
    "    Model Schema is based on \n",
    "    https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3\n",
    "    ImageNet Pretrained Weights \n",
    "    https://drive.google.com/file/d/0Bz7KyqmuGsilT0J5dmRCM0ROVHc/view?usp=sharing\n",
    "    Parameters:\n",
    "      img_rows, img_cols - resolution of inputs\n",
    "      channel - 1 for grayscale, 3 for color \n",
    "      num_class - number of class labels for our classification task\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=(channel, img_rows, img_cols)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1, 1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "    # Add Fully Connected Layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    # Loads ImageNet pre-trained data\n",
    "    model.load_weights('../cache/vgg16_weights.h5')\n",
    "\n",
    "    # Truncate and replace softmax layer for transfer learning\n",
    "    model.layers.pop()\n",
    "    model.outputs = [model.layers[-1].output]\n",
    "    model.layers[-1].outbound_nodes = []\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "\n",
    "    # Uncomment below to set all but the last 10 layers to non-trainable (weights will not be updated)\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[8:]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    # Learning rate is changed to 0.001\n",
    "    sgd = SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "def preprocess_input(x):\n",
    "    #resnet50 image preprocessing\n",
    "    # 'RGB'->'BGR'\n",
    "    x = x[:, :, ::-1]\n",
    "    x[:, :, 0] -= 100\n",
    "    x[:, :, 1] -= 115\n",
    "    x[:, :, 2] -= 124\n",
    "    return x\n",
    "\n",
    "def create_rect5(row):\n",
    "    if is_seal:\n",
    "        return plt.Rectangle((row['x0'], row['y0']), row['w'], row['h'], color='red', fill=False, lw=2)\n",
    "    else:\n",
    "        return plt.Rectangle((row['x0'], row['y0']), row['w'], row['h'], color='red', fill=False, lw=4)\n",
    "\n",
    "def load_img(path, bbox, target_size=None):\n",
    "    img = Image.open(path)\n",
    "    img = img.convert('RGB')\n",
    "    cropped = img.crop((bbox[0],bbox[1],bbox[2],bbox[3]))\n",
    "    if target_size:\n",
    "        cropped = cropped.resize((target_size[1], target_size[0]))\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "img_rows, img_cols = 224, 224 # Resolution of inputs\n",
    "channel = 3\n",
    "num_class = 2\n",
    "ROWS, COLS = 224, 224\n",
    "BATCHSIZE = 128\n",
    "SEAL_CLASSES = ['NoS', 'seal']\n",
    "nb_perClass = int(BATCHSIZE / len(SEAL_CLASSES))\n",
    "TRAIN_DIR = '../darknet/seals/JPEGImagesBlk'\n",
    "TEST_DIR = '../darknet/seals/JPEGImagesTest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>proba</th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>seal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>228_54</td>\n",
       "      <td>0.894</td>\n",
       "      <td>364.95</td>\n",
       "      <td>295.10</td>\n",
       "      <td>464.95</td>\n",
       "      <td>395.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>228_54</td>\n",
       "      <td>0.893</td>\n",
       "      <td>228.20</td>\n",
       "      <td>376.30</td>\n",
       "      <td>328.20</td>\n",
       "      <td>476.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>228_54</td>\n",
       "      <td>0.720</td>\n",
       "      <td>407.75</td>\n",
       "      <td>409.95</td>\n",
       "      <td>507.75</td>\n",
       "      <td>509.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      img  proba      x0      y0      x1      y1  seal\n",
       "0  228_54  0.894  364.95  295.10  464.95  395.10     0\n",
       "1  228_54  0.893  228.20  376.30  328.20  476.30     0\n",
       "2  228_54  0.720  407.75  409.95  507.75  509.95     0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune Example\n",
    "rfcnCV = pd.read_pickle('../coords/rfcnCV.pkl')\n",
    "rfcnCV.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>proba</th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13835_33</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.00</td>\n",
       "      <td>444.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>544.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11555_64</td>\n",
       "      <td>0.704</td>\n",
       "      <td>444.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>544.00</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11710_62</td>\n",
       "      <td>0.977</td>\n",
       "      <td>23.65</td>\n",
       "      <td>41.4</td>\n",
       "      <td>123.65</td>\n",
       "      <td>141.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11710_62</td>\n",
       "      <td>0.963</td>\n",
       "      <td>444.00</td>\n",
       "      <td>12.0</td>\n",
       "      <td>544.00</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17920_13</td>\n",
       "      <td>0.939</td>\n",
       "      <td>3.85</td>\n",
       "      <td>444.0</td>\n",
       "      <td>103.85</td>\n",
       "      <td>544.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        img  proba      x0     y0      x1     y1\n",
       "0  13835_33  0.967    0.00  444.0  100.00  544.0\n",
       "1  11555_64  0.704  444.00    0.0  544.00  100.0\n",
       "2  11710_62  0.977   23.65   41.4  123.65  141.4\n",
       "3  11710_62  0.963  444.00   12.0  544.00  112.0\n",
       "4  17920_13  0.939    3.85  444.0  103.85  544.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune Example\n",
    "test_df = pd.read_pickle('../coords/rfcnTst.pkl')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_generator(datagen, df):\n",
    "    while 1:\n",
    "        batch_x = np.zeros((BATCHSIZE, ROWS, COLS, 3), dtype=K.floatx())\n",
    "        batch_y = np.zeros((BATCHSIZE, len(SEAL_CLASSES)), dtype=K.floatx())\n",
    "        fn = lambda obj: obj.loc[np.random.choice(obj.index, size=nb_perClass, replace=False),:]\n",
    "        batch_df = df.groupby(['seal'], as_index=True).apply(fn)\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            row = row.tolist()\n",
    "            image_file = os.path.join(TRAIN_DIR, row[0])\n",
    "            seal = row[6]\n",
    "            bbox = row[2:6]\n",
    "            cropped = load_img(image_file+'.jpg',bbox,target_size=(ROWS,COLS))\n",
    "            x = np.asarray(cropped, dtype=K.floatx())\n",
    "            x = datagen.random_transform(x)\n",
    "            x = preprocess_input(x)\n",
    "            batch_x[i] = x\n",
    "            batch_y[i,seal] = 1\n",
    "            i += 1\n",
    "        yield (batch_x.transpose(0, 3, 1, 2), batch_y)\n",
    "\n",
    "def test_generator(df, datagen = None, batch_size = BATCHSIZE):\n",
    "    n = df.shape[0]\n",
    "    batch_index = 0\n",
    "    while 1:\n",
    "        current_index = batch_index * batch_size\n",
    "        if n >= current_index + batch_size:\n",
    "            current_batch_size = batch_size\n",
    "            batch_index += 1    \n",
    "        else:\n",
    "            current_batch_size = n - current_index\n",
    "            batch_index = 0        \n",
    "        batch_df = df[current_index:current_index+current_batch_size]\n",
    "        batch_x = np.zeros((batch_df.shape[0], ROWS, COLS, 3), dtype=K.floatx())\n",
    "        i = 0\n",
    "        for index,row in batch_df.iterrows():\n",
    "            row = row.tolist()\n",
    "            image_file = os.path.join(TEST_DIR, row[0]+'.jpg')\n",
    "            bbox = row[2:6]\n",
    "            cropped = load_img(image_file,bbox,target_size=(ROWS,COLS))\n",
    "            x = np.asarray(cropped, dtype=K.floatx())\n",
    "            if datagen is not None: x = datagen.random_transform(x)            \n",
    "            x = preprocess_input(x)\n",
    "            batch_x[i] = x\n",
    "            i += 1\n",
    "        if batch_index%50 == 0: print(batch_index)\n",
    "        #return(batch_x.transpose(0, 3, 1, 2))\n",
    "        yield(batch_x.transpose(0, 3, 1, 2))\n",
    "        \n",
    "# Data generator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4286, 3, 224, 224)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets make our validation set\n",
    "CVsplit = rfcnCV.img.str.split('_').apply(lambda x: x[0]).astype(int) % 40 == 0\n",
    "train_df = rfcnCV[~CVsplit]\n",
    "valid_df = rfcnCV[CVsplit]\n",
    "\n",
    "# validation_data (valid_x,valid_y)\n",
    "df_1 = valid_df\n",
    "l = valid_df.groupby('seal').size()\n",
    "nb_NoF_valid = math.ceil(l.sum()/10)\n",
    "valid_x = np.zeros((valid_df.shape[0], ROWS, COLS, 3), dtype=K.floatx())\n",
    "valid_y = np.zeros((valid_df.shape[0], len(SEAL_CLASSES)), dtype=K.floatx())\n",
    "i = 0\n",
    "for index,row in valid_df.iterrows():\n",
    "    row = row.tolist()\n",
    "    image_file = os.path.join(TRAIN_DIR, row[0])\n",
    "    seal = row[6]\n",
    "    bbox = row[2:6]\n",
    "    cropped = load_img(image_file+'.jpg',bbox,target_size=(ROWS,COLS))\n",
    "    x = np.asarray(cropped, dtype=K.floatx())\n",
    "    x = preprocess_input(x)\n",
    "    valid_x[i] = x\n",
    "    valid_y[i,seal] = 1\n",
    "    i += 1\n",
    "valid_x = valid_x.transpose(0, 3, 1, 2)\n",
    "valid_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "69888/70000 [============================>.] - ETA: 6s - loss: 0.1599 - acc: 0.9415 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.py:1573: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70016/70000 [==============================] - 4185s - loss: 0.1598 - acc: 0.9416 - val_loss: 0.1021 - val_acc: 0.9624\n",
      "Epoch 2/2\n",
      "70016/70000 [==============================] - 4189s - loss: 0.1056 - acc: 0.9624 - val_loss: 0.0862 - val_acc: 0.9690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f96b7c96490>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load our model\n",
    "nb_epoch = 2\n",
    "samples_per_epoch = 70000\n",
    "model = vgg_std16_model(ROWS, COLS, channel, num_class)\n",
    "\n",
    "# Start Fine-tuning\n",
    "model.fit_generator(train_generator(train_datagen, train_df),\n",
    "          nb_epoch=nb_epoch,\n",
    "          samples_per_epoch=samples_per_epoch, #50000,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x, valid_y),\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "70016/70000 [==============================] - 4190s - loss: 0.0899 - acc: 0.9680 - val_loss: 0.0851 - val_acc: 0.9683\n",
      "Epoch 2/3\n",
      "70016/70000 [==============================] - 4186s - loss: 0.0818 - acc: 0.9714 - val_loss: 0.0812 - val_acc: 0.9701\n",
      "Epoch 3/3\n",
      "70016/70000 [==============================] - 4184s - loss: 0.0725 - acc: 0.9733 - val_loss: 0.0748 - val_acc: 0.9729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f95fc3a5150>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in model.layers[10:]:\n",
    "    layer.trainable = True\n",
    "model.optimizer.lr = 1e-4\n",
    "nb_epoch = 3\n",
    "model.fit_generator(train_generator(train_datagen, df=train_df),\n",
    "          nb_epoch=nb_epoch,\n",
    "          samples_per_epoch=samples_per_epoch,\n",
    "          verbose=1,\n",
    "          validation_data=(valid_x, valid_y),\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n",
      "800\n",
      "850\n",
      "900\n",
      "950\n",
      "1000\n",
      "1050\n",
      "1100\n",
      "1150\n",
      "1200\n",
      "1250\n",
      "1300\n",
      "1350\n",
      "1400\n",
      "1450\n",
      "1500\n",
      "1550\n",
      "1600\n",
      "1650\n",
      "1700\n",
      "1750\n",
      "1800\n",
      "1850\n",
      "1900\n",
      "1950\n",
      "2000\n",
      "2050\n",
      "2100\n",
      "2150\n",
      "2200\n",
      "2250\n",
      "2300\n",
      "2350\n",
      "2400\n",
      "2450\n",
      "2500\n",
      "2550\n",
      "2600\n",
      "2650\n",
      "2700\n",
      "2750\n",
      "2800\n",
      "2850\n",
      "2900\n",
      "2950\n",
      "3000\n",
      "3050\n",
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n",
      "6800\n",
      "6850\n",
      "6900\n",
      "6950\n",
      "7000\n",
      "7050\n",
      "7100\n",
      "7150\n",
      "7200\n",
      "7250\n",
      "7300\n",
      "7350\n",
      "7400\n",
      "7450\n",
      "7500\n",
      "7550\n",
      "7600\n",
      "7650\n",
      "7700\n",
      "7750\n",
      "7800\n",
      "7850\n",
      "7900\n",
      "7950\n",
      "8000\n",
      "8050\n",
      "8100\n",
      "8150\n",
      "8200\n",
      "8250\n",
      "8300\n",
      "8350\n",
      "8400\n",
      "8450\n",
      "8500\n",
      "8550\n",
      "8600\n",
      "8650\n",
      "8700\n",
      "8750\n",
      "8800\n",
      "8850\n",
      "8900\n",
      "8950\n",
      "9000\n",
      "9050\n",
      "9100\n",
      "9150\n",
      "9200\n",
      "9250\n",
      "9300\n",
      "9350\n",
      "9400\n",
      "9450\n",
      "9500\n",
      "9550\n",
      "9600\n",
      "9650\n",
      "9700\n",
      "9750\n",
      "9800\n",
      "9850\n",
      "9900\n",
      "9950\n",
      "10000\n",
      "10050\n",
      "10100\n",
      "10150\n",
      "10200\n",
      "10250\n",
      "10300\n",
      "10350\n",
      "10400\n",
      "10450\n",
      "10500\n",
      "10550\n",
      "10600\n",
      "10650\n",
      "10700\n",
      "10750\n",
      "10800\n",
      "10850\n",
      "10900\n",
      "10950\n",
      "11000\n",
      "11050\n",
      "11100\n",
      "11150\n",
      "11200\n",
      "11250\n",
      "11300\n",
      "11350\n",
      "11400\n",
      "11450\n",
      "11500\n",
      "11550\n",
      "11600\n",
      "11650\n",
      "11700\n",
      "11750\n",
      "11800\n",
      "11850\n",
      "11900\n",
      "11950\n",
      "12000\n",
      "12050\n",
      "12100\n",
      "12150\n",
      "12200\n",
      "12250\n",
      "12300\n",
      "12350\n",
      "12400\n",
      "12450\n",
      "12500\n",
      "12550\n",
      "12600\n",
      "12650\n",
      "12700\n",
      "12750\n",
      "12800\n",
      "12850\n",
      "12900\n",
      "12950\n",
      "13000\n",
      "13050\n",
      "13100\n",
      "13150\n",
      "13200\n",
      "13250\n",
      "13300\n",
      "13350\n",
      "13400\n",
      "13450\n",
      "13500\n",
      "13550\n",
      "13600\n",
      "13650\n",
      "13700\n",
      "13750\n",
      "13800\n",
      "13850\n",
      "13900\n",
      "13950\n",
      "14000\n",
      "14050\n",
      "14100\n",
      "14150\n",
      "14200\n",
      "14250\n",
      "14300\n",
      "14350\n",
      "14400\n",
      "14450\n",
      "14500\n",
      "14550\n",
      "14600\n",
      "14650\n",
      "14700\n",
      "14750\n",
      "14800\n",
      "14850\n",
      "14900\n",
      "14950\n",
      "15000\n",
      "15050\n",
      "15100\n",
      "15150\n",
      "15200\n",
      "15250\n",
      "15300\n",
      "15350\n",
      "15400\n",
      "15450\n",
      "15500\n",
      "15550\n",
      "15600\n",
      "15650\n",
      "15700\n",
      "15750\n",
      "15800\n",
      "15850\n",
      "15900\n",
      "15950\n",
      "16000\n",
      "16050\n",
      "16100\n",
      "16150\n",
      "16200\n",
      "16250\n",
      "16300\n",
      "16350\n",
      "16400\n",
      "16450\n",
      "16500\n",
      "16550\n",
      "16600\n",
      "16650\n",
      "16700\n",
      "16750\n",
      "16800\n",
      "16850\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# test_preds = test_model.predict_generator(test_generator(test_df), val_samples=test_df.shape[0]))\n",
    "test_preds = model.predict_generator(test_generator(test_df), val_samples=test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.concat([test_df, pd.DataFrame(test_preds,  columns=['predNoSeal', 'predSeal'])], axis=1)\n",
    "df.to_pickle('../coords/vggPreds2604.pkl')\n",
    "df.to_csv('../coords/vggTestPreds2604.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>proba</th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>predNoSeal</th>\n",
       "      <th>predSeal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13835_33</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.00</td>\n",
       "      <td>444.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>0.955897</td>\n",
       "      <td>0.044103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11555_64</td>\n",
       "      <td>0.704</td>\n",
       "      <td>444.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.957744</td>\n",
       "      <td>0.042256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11710_62</td>\n",
       "      <td>0.977</td>\n",
       "      <td>23.65</td>\n",
       "      <td>41.40</td>\n",
       "      <td>123.65</td>\n",
       "      <td>141.40</td>\n",
       "      <td>0.944988</td>\n",
       "      <td>0.055012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11710_62</td>\n",
       "      <td>0.963</td>\n",
       "      <td>444.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>112.00</td>\n",
       "      <td>0.258724</td>\n",
       "      <td>0.741276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17920_13</td>\n",
       "      <td>0.939</td>\n",
       "      <td>3.85</td>\n",
       "      <td>444.00</td>\n",
       "      <td>103.85</td>\n",
       "      <td>544.00</td>\n",
       "      <td>0.211189</td>\n",
       "      <td>0.788811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17920_13</td>\n",
       "      <td>0.796</td>\n",
       "      <td>254.65</td>\n",
       "      <td>444.00</td>\n",
       "      <td>354.65</td>\n",
       "      <td>544.00</td>\n",
       "      <td>0.997278</td>\n",
       "      <td>0.002722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11515_60</td>\n",
       "      <td>0.908</td>\n",
       "      <td>158.70</td>\n",
       "      <td>15.10</td>\n",
       "      <td>258.70</td>\n",
       "      <td>115.10</td>\n",
       "      <td>0.998412</td>\n",
       "      <td>0.001588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11515_60</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.999490</td>\n",
       "      <td>0.000510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13368_45</td>\n",
       "      <td>0.934</td>\n",
       "      <td>240.35</td>\n",
       "      <td>169.55</td>\n",
       "      <td>340.35</td>\n",
       "      <td>269.55</td>\n",
       "      <td>0.999361</td>\n",
       "      <td>0.000639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13368_45</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.00</td>\n",
       "      <td>232.95</td>\n",
       "      <td>100.00</td>\n",
       "      <td>332.95</td>\n",
       "      <td>0.999308</td>\n",
       "      <td>0.000692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13368_45</td>\n",
       "      <td>0.712</td>\n",
       "      <td>444.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.999380</td>\n",
       "      <td>0.000620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13368_45</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.999485</td>\n",
       "      <td>0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11704_84</td>\n",
       "      <td>0.969</td>\n",
       "      <td>279.15</td>\n",
       "      <td>330.15</td>\n",
       "      <td>379.15</td>\n",
       "      <td>430.15</td>\n",
       "      <td>0.999251</td>\n",
       "      <td>0.000749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11704_84</td>\n",
       "      <td>0.896</td>\n",
       "      <td>242.10</td>\n",
       "      <td>181.10</td>\n",
       "      <td>342.10</td>\n",
       "      <td>281.10</td>\n",
       "      <td>0.860371</td>\n",
       "      <td>0.139629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11704_84</td>\n",
       "      <td>0.828</td>\n",
       "      <td>188.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>288.10</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.999441</td>\n",
       "      <td>0.000559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11704_84</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.999811</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7121_51</td>\n",
       "      <td>0.989</td>\n",
       "      <td>55.25</td>\n",
       "      <td>424.95</td>\n",
       "      <td>155.25</td>\n",
       "      <td>524.95</td>\n",
       "      <td>0.393549</td>\n",
       "      <td>0.606451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7121_51</td>\n",
       "      <td>0.909</td>\n",
       "      <td>336.45</td>\n",
       "      <td>444.00</td>\n",
       "      <td>436.45</td>\n",
       "      <td>544.00</td>\n",
       "      <td>0.989985</td>\n",
       "      <td>0.010015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7121_51</td>\n",
       "      <td>0.803</td>\n",
       "      <td>386.00</td>\n",
       "      <td>444.00</td>\n",
       "      <td>486.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>0.999380</td>\n",
       "      <td>0.000620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7121_51</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.982477</td>\n",
       "      <td>0.017523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15385_14</td>\n",
       "      <td>0.720</td>\n",
       "      <td>444.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.998889</td>\n",
       "      <td>0.001111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15385_14</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.999617</td>\n",
       "      <td>0.000383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15778_83</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.00</td>\n",
       "      <td>444.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>0.380784</td>\n",
       "      <td>0.619216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15778_83</td>\n",
       "      <td>0.970</td>\n",
       "      <td>444.00</td>\n",
       "      <td>218.40</td>\n",
       "      <td>544.00</td>\n",
       "      <td>318.40</td>\n",
       "      <td>0.126016</td>\n",
       "      <td>0.873984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15778_83</td>\n",
       "      <td>0.935</td>\n",
       "      <td>101.00</td>\n",
       "      <td>229.25</td>\n",
       "      <td>201.00</td>\n",
       "      <td>329.25</td>\n",
       "      <td>0.009017</td>\n",
       "      <td>0.990983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15778_83</td>\n",
       "      <td>0.927</td>\n",
       "      <td>102.75</td>\n",
       "      <td>285.70</td>\n",
       "      <td>202.75</td>\n",
       "      <td>385.70</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>0.996592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15778_83</td>\n",
       "      <td>0.888</td>\n",
       "      <td>444.00</td>\n",
       "      <td>444.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>0.992163</td>\n",
       "      <td>0.007837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15778_83</td>\n",
       "      <td>0.882</td>\n",
       "      <td>285.15</td>\n",
       "      <td>301.60</td>\n",
       "      <td>385.15</td>\n",
       "      <td>401.60</td>\n",
       "      <td>0.019686</td>\n",
       "      <td>0.980314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15778_83</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.579226</td>\n",
       "      <td>0.420774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7185_85</td>\n",
       "      <td>0.949</td>\n",
       "      <td>0.00</td>\n",
       "      <td>259.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>359.00</td>\n",
       "      <td>0.999664</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7185_85</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.999619</td>\n",
       "      <td>0.000381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5049_24</td>\n",
       "      <td>0.729</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.999248</td>\n",
       "      <td>0.000752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>228_54</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.999825</td>\n",
       "      <td>0.000175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>228_54</td>\n",
       "      <td>0.722</td>\n",
       "      <td>444.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.999848</td>\n",
       "      <td>0.000152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>17146_84</td>\n",
       "      <td>0.884</td>\n",
       "      <td>79.05</td>\n",
       "      <td>175.45</td>\n",
       "      <td>179.05</td>\n",
       "      <td>275.45</td>\n",
       "      <td>0.021904</td>\n",
       "      <td>0.978096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>17146_84</td>\n",
       "      <td>0.763</td>\n",
       "      <td>354.65</td>\n",
       "      <td>161.35</td>\n",
       "      <td>454.65</td>\n",
       "      <td>261.35</td>\n",
       "      <td>0.998566</td>\n",
       "      <td>0.001434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>17146_84</td>\n",
       "      <td>0.733</td>\n",
       "      <td>354.70</td>\n",
       "      <td>201.80</td>\n",
       "      <td>454.70</td>\n",
       "      <td>301.80</td>\n",
       "      <td>0.998566</td>\n",
       "      <td>0.001434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>854_02</td>\n",
       "      <td>0.996</td>\n",
       "      <td>106.35</td>\n",
       "      <td>444.00</td>\n",
       "      <td>206.35</td>\n",
       "      <td>544.00</td>\n",
       "      <td>0.036995</td>\n",
       "      <td>0.963005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>854_02</td>\n",
       "      <td>0.980</td>\n",
       "      <td>444.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>544.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.981901</td>\n",
       "      <td>0.018099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>854_02</td>\n",
       "      <td>0.845</td>\n",
       "      <td>281.20</td>\n",
       "      <td>117.40</td>\n",
       "      <td>381.20</td>\n",
       "      <td>217.40</td>\n",
       "      <td>0.989491</td>\n",
       "      <td>0.010509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>854_02</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.987446</td>\n",
       "      <td>0.012554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>11508_70</td>\n",
       "      <td>0.840</td>\n",
       "      <td>401.85</td>\n",
       "      <td>444.00</td>\n",
       "      <td>501.85</td>\n",
       "      <td>544.00</td>\n",
       "      <td>0.979750</td>\n",
       "      <td>0.020250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4112_02</td>\n",
       "      <td>0.976</td>\n",
       "      <td>61.85</td>\n",
       "      <td>323.20</td>\n",
       "      <td>161.85</td>\n",
       "      <td>423.20</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>0.000380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4112_02</td>\n",
       "      <td>0.951</td>\n",
       "      <td>175.00</td>\n",
       "      <td>357.35</td>\n",
       "      <td>275.00</td>\n",
       "      <td>457.35</td>\n",
       "      <td>0.999595</td>\n",
       "      <td>0.000405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4112_02</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.999458</td>\n",
       "      <td>0.000542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>13740_00</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.997926</td>\n",
       "      <td>0.002074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>11103_72</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.998975</td>\n",
       "      <td>0.001025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>11243_80</td>\n",
       "      <td>0.798</td>\n",
       "      <td>403.05</td>\n",
       "      <td>444.00</td>\n",
       "      <td>503.05</td>\n",
       "      <td>544.00</td>\n",
       "      <td>0.999234</td>\n",
       "      <td>0.000766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>14826_72</td>\n",
       "      <td>0.758</td>\n",
       "      <td>444.00</td>\n",
       "      <td>306.35</td>\n",
       "      <td>544.00</td>\n",
       "      <td>406.35</td>\n",
       "      <td>0.803058</td>\n",
       "      <td>0.196942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>14826_72</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.999596</td>\n",
       "      <td>0.000404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         img  proba      x0      y0      x1      y1  predNoSeal  predSeal\n",
       "0   13835_33  0.967    0.00  444.00  100.00  544.00    0.955897  0.044103\n",
       "1   11555_64  0.704  444.00    0.00  544.00  100.00    0.957744  0.042256\n",
       "2   11710_62  0.977   23.65   41.40  123.65  141.40    0.944988  0.055012\n",
       "3   11710_62  0.963  444.00   12.00  544.00  112.00    0.258724  0.741276\n",
       "4   17920_13  0.939    3.85  444.00  103.85  544.00    0.211189  0.788811\n",
       "5   17920_13  0.796  254.65  444.00  354.65  544.00    0.997278  0.002722\n",
       "6   11515_60  0.908  158.70   15.10  258.70  115.10    0.998412  0.001588\n",
       "7   11515_60  0.703    0.00    0.00  100.00  100.00    0.999490  0.000510\n",
       "8   13368_45  0.934  240.35  169.55  340.35  269.55    0.999361  0.000639\n",
       "9   13368_45  0.918    0.00  232.95  100.00  332.95    0.999308  0.000692\n",
       "10  13368_45  0.712  444.00    0.00  544.00  100.00    0.999380  0.000620\n",
       "11  13368_45  0.707    0.00    0.00  100.00  100.00    0.999485  0.000515\n",
       "12  11704_84  0.969  279.15  330.15  379.15  430.15    0.999251  0.000749\n",
       "13  11704_84  0.896  242.10  181.10  342.10  281.10    0.860371  0.139629\n",
       "14  11704_84  0.828  188.10    0.00  288.10  100.00    0.999441  0.000559\n",
       "15  11704_84  0.760    0.00    0.00  100.00  100.00    0.999811  0.000189\n",
       "16   7121_51  0.989   55.25  424.95  155.25  524.95    0.393549  0.606451\n",
       "17   7121_51  0.909  336.45  444.00  436.45  544.00    0.989985  0.010015\n",
       "18   7121_51  0.803  386.00  444.00  486.00  544.00    0.999380  0.000620\n",
       "19   7121_51  0.761    0.00    0.00  100.00  100.00    0.982477  0.017523\n",
       "20  15385_14  0.720  444.00    0.00  544.00  100.00    0.998889  0.001111\n",
       "21  15385_14  0.718    0.00    0.00  100.00  100.00    0.999617  0.000383\n",
       "22  15778_83  0.982    0.00  444.00  100.00  544.00    0.380784  0.619216\n",
       "23  15778_83  0.970  444.00  218.40  544.00  318.40    0.126016  0.873984\n",
       "24  15778_83  0.935  101.00  229.25  201.00  329.25    0.009017  0.990983\n",
       "25  15778_83  0.927  102.75  285.70  202.75  385.70    0.003408  0.996592\n",
       "26  15778_83  0.888  444.00  444.00  544.00  544.00    0.992163  0.007837\n",
       "27  15778_83  0.882  285.15  301.60  385.15  401.60    0.019686  0.980314\n",
       "28  15778_83  0.722    0.00    0.00  100.00  100.00    0.579226  0.420774\n",
       "29   7185_85  0.949    0.00  259.00  100.00  359.00    0.999664  0.000336\n",
       "30   7185_85  0.795    0.00    0.00  100.00  100.00    0.999619  0.000381\n",
       "31   5049_24  0.729    0.00    0.00  100.00  100.00    0.999248  0.000752\n",
       "32    228_54  0.800    0.00    0.00  100.00  100.00    0.999825  0.000175\n",
       "33    228_54  0.722  444.00    0.00  544.00  100.00    0.999848  0.000152\n",
       "34  17146_84  0.884   79.05  175.45  179.05  275.45    0.021904  0.978096\n",
       "35  17146_84  0.763  354.65  161.35  454.65  261.35    0.998566  0.001434\n",
       "36  17146_84  0.733  354.70  201.80  454.70  301.80    0.998566  0.001434\n",
       "37    854_02  0.996  106.35  444.00  206.35  544.00    0.036995  0.963005\n",
       "38    854_02  0.980  444.00    0.00  544.00  100.00    0.981901  0.018099\n",
       "39    854_02  0.845  281.20  117.40  381.20  217.40    0.989491  0.010509\n",
       "40    854_02  0.844    0.00    0.00  100.00  100.00    0.987446  0.012554\n",
       "41  11508_70  0.840  401.85  444.00  501.85  544.00    0.979750  0.020250\n",
       "42   4112_02  0.976   61.85  323.20  161.85  423.20    0.999620  0.000380\n",
       "43   4112_02  0.951  175.00  357.35  275.00  457.35    0.999595  0.000405\n",
       "44   4112_02  0.802    0.00    0.00  100.00  100.00    0.999458  0.000542\n",
       "45  13740_00  0.813    0.00    0.00  100.00  100.00    0.997926  0.002074\n",
       "46  11103_72  0.759    0.00    0.00  100.00  100.00    0.998975  0.001025\n",
       "47  11243_80  0.798  403.05  444.00  503.05  544.00    0.999234  0.000766\n",
       "48  14826_72  0.758  444.00  306.35  544.00  406.35    0.803058  0.196942\n",
       "49  14826_72  0.746    0.00    0.00  100.00  100.00    0.999596  0.000404"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           zeropadding2d_input_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  1792        zeropadding2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  36928       zeropadding2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 73856       zeropadding2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 147584      zeropadding2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   295168      zeropadding2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   1180160     zeropadding2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   2359808     zeropadding2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 512, 7, 7)     0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25088)         0           maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 4096)          102764544   flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4096)          0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4096)          16781312    dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 4096)          0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 2)             8194        dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 134,156,162\n",
      "Non-trainable params: 112,576\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
